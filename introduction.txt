# カスタムLLM実装プロジェクト：初心者向けマニュアル

## 1. プロジェクト概要

このプロジェクトは、GPT（Generative Pre-trained Transformer）アーキテクチャをベースにしたカスタムLLM（Large Language Model）の実装です。OpenWebTextデータセットを使用して学習を行い、テキスト生成タスクを実行します。このプロジェクトを通じて、LLMの基本的な仕組みと実装方法を学ぶことができます。

## 2. 環境設定とインストール手順

### 2.1 必要なソフトウェア

- Python 3.11以上
- Poetry（Pythonパッケージ管理ツール）
- Git（バージョン管理システム）

### 2.2 インストール手順

1. Pythonをインストールします（https://www.python.org/downloads/）。

2. コマンドラインで以下のコマンドを実行し、Poetryをインストールします：

```
pip install poetry
```

3. プロジェクトをクローンします：

```
git clone [プロジェクトのURL]
cd [プロジェクトディレクトリ]
```

4. 依存関係をインストールします：

```
poetry install
```

## 3. プロジェクト構造

```
my-llm-project/
├── scripts/
│   └── train.py
├── src/
│   ├── data_processing.py
│   ├── model.py
│   ├── training.py
│   └── utils.py
├── Dockerfile
├── Makefile
├── pyproject.toml
├── README.md
└── compose.yaml
```

## 4. データの準備と前処理

データの準備と前処理を行うには、以下のコマンドを実行します：

```
poetry run python -c "from src.data_processing import load_and_process_dataset, save_processed_dataset, create_memmap_files; tokenized, _ = load_and_process_dataset(); save_processed_dataset(tokenized); create_memmap_files(tokenized)"
```

このコマンドは、OpenWebTextデータセットをダウンロードし、トークン化して保存します。

## 5. モデルの学習

モデルの学習を開始するには、以下のコマンドを実行します：

```
poetry run python scripts/train.py
```

学習プロセスが開始され、進捗状況がコンソールに表示されます。

## 6. テキスト生成（学習後）

学習したモデルを使用してテキストを生成するには、以下のコマンドを実行します：

```
docker-compose run llm poetry run python -c "import torch; from src.model import GPT; model = GPT(...); model.load_state_dict(torch.load('best_checkpoint.bin')['model']); print(model.generate_sentence('Once upon a time', 1024, 100, tokenizer, 'cpu'))"
```

このコマンドは、「Once upon a time」から始まる文章を生成します。

## 7. パラメーターの調整方法とその影響

主要なパラメーターとその影響は以下の通りです：

### 7.1 embedding_size（埋め込みサイズ）
- 現在の値：768
- 影響：大きくすると、モデルの表現力が増しますが、計算コストも増加します。
- 調整方法：`scripts/train.py`の`config`辞書内で変更します。

### 7.2 num_heads（アテンションヘッドの数）
- 現在の値：6
- 影響：増やすと、モデルが異なる種類の情報に注目できるようになりますが、計算コストが増加します。
- 調整方法：`scripts/train.py`の`config`辞書内で変更します。

### 7.3 depth（モデルの深さ）
- 現在の値：6
- 影響：深くすると、モデルの複雑性が増しますが、学習が難しくなる可能性があります。
- 調整方法：`scripts/train.py`の`config`辞書内で変更します。

### 7.4 batch_size（バッチサイズ）
- 現在の値：6
- 影響：大きくすると学習が速くなりますが、メモリ使用量が増加します。
- 調整方法：`scripts/train.py`の`config`辞書内で変更します。

### 7.5 max_lr（最大学習率）
- 現在の値：2.5e-5
- 影響：大きすぎると学習が不安定になり、小さすぎると学習が遅くなります。
- 調整方法：`scripts/train.py`の`config`辞書内で変更します。

これらのパラメーターを調整する際は、一度に1つずつ変更し、その影響を観察することをお勧めします。

## 8. トラブルシューティング

### 8.1 メモリ不足エラー
症状：`RuntimeError: CUDA out of memory`
解決策：batch_sizeを小さくするか、より少ないGPUメモリを使用するモデル設定に変更します。

### 8.2 学習が進まない
症状：損失が減少しない
解決策：学習率（max_lr, min_lr）を調整するか、モデルのアーキテクチャ（depth, num_heads）を変更してみてください。

### 8.3 データ準備エラー
症状：`FileNotFoundError: [Errno 2] No such file or directory: 'train.bin'`
解決策：データの準備と前処理のステップ（セクション4）を正しく実行したか確認してください。

### 8.4 CUDA関連のエラー
症状：`torch.cuda.is_available()` が `False` を返す
解決策：CUDA対応のGPUが利用可能か確認し、必要に応じてCUDAドライバーをアップデートしてください。

## 9. まとめ

このプロジェクトを通じて、LLMの基本的な実装と学習プロセスを体験できます。パラメーターを調整しながら、モデルの挙動の変化を観察することで、深層学習モデルの動作原理をより深く理解することができます。

実験を重ね、エラーに直面し、それを解決していく過程で、機械学習プロジェクトの実践的なスキルを身につけることができるでしょう。

ぜひ、このプロジェクトを足がかりに、さらに高度なNLP（自然言語処理）タスクや大規模言語モデルの研究に挑戦してみてください。

Citations:
[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/21455164/e82becfc-5f06-4b3d-97f3-672fa1c92297/create_llm_project_summary.txt
[2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/21455164/0e4e533f-9d61-404e-9620-460ed4c88594/create_llm_project_summary.txt
